### Importing libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
### Load NSL-KDD dataset

url = "http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz"
column_names = ["duration", "protocol_type", "service", "flag", "src_bytes",
                "dst_bytes", "land", "wrong_fragment", "urgent", "hot",
                "num_failed_logins", "logged_in", "num_compromised", "root_shell",
                "su_attempted", "num_root", "num_file_creations", "num_shells",
                "num_access_files", "num_outbound_cmds", "is_host_login",
                "is_guest_login", "count", "srv_count", "serror_rate",
                "srv_serror_rate", "rerror_rate", "srv_rerror_rate",
                "same_srv_rate", "diff_srv_rate", "srv_diff_host_rate",
                "dst_host_count", "dst_host_srv_count", "dst_host_same_srv_rate",
                "dst_host_diff_srv_rate", "dst_host_same_src_port_rate",
                "dst_host_srv_diff_host_rate", "dst_host_serror_rate",
                "dst_host_srv_serror_rate", "dst_host_rerror_rate",
                "dst_host_srv_rerror_rate", "label"]
df = pd.read_csv(url, names=column_names)
df.head()
### Drop non-numeric columns and label column
# Drop non-numeric columns and label column
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
df_numeric = df[numeric_columns]
### Heatmap of Correlation between Features

plt.figure(figsize=(30, 20))
sns.heatmap(df_numeric.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Heatmap of Correlation between Features')
plt.show()
### Data Pre-processing
# Standardize the data
scaler = StandardScaler()
X = scaler.fit_transform(df_numeric)

# Split the data into training and test sets
X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)
### Define the autoencoder model
model = Sequential([
    Dense(20, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(10, activation='relu'),
    Dense(20, activation='relu'),
    Dense(X_train.shape[1])
])

# Compile the model
model.compile(optimizer='adam', loss='mse')
### Train the autoencoder
history = model.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_test, X_test))
### Training and Validation Loss Over Epochs
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

### Evaluating the model
# Use the trained autoencoder to encode and decode the test data
encoded_data = model.predict(X_test)

# Evaluate the reconstruction error
mse = np.mean(np.square(X_test - encoded_data))
print("Mean Squared Error:", mse)
### Function to detect anomalies
# Function to detect anomalies
def detect_anomalies(X_test, threshold):
    reconstructed = model.predict(X_test)
    reconstruction_errors = np.mean(np.power(X_test - reconstructed, 2), axis=1)
    anomaly_indices = np.where(reconstruction_errors > threshold)[0]
    return anomaly_indices, reconstruction_errors
threshold = 0.3
anomaly_indices, reconstruction_errors = detect_anomalies(X_test, threshold)
print("Anomaly indices:", anomaly_indices)
print("Reconstruction errors:", reconstruction_errors)
# Train set predictions
train_pred = model.predict(X_train)
train_mse = np.mean(np.square(X_train - train_pred))

# Test set predictions
test_pred = model.predict(X_test)
test_mse = np.mean(np.square(X_test - test_pred))

print("Training Mean Squared Error:", train_mse)
print("Testing Mean Squared Error:", test_mse)
### KNN model for comparison
from sklearn.cluster import KMeans
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(df_numeric)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()
from sklearn.neighbors import NearestNeighbors


# Standardizing the data
scaler_knn = StandardScaler()
X_scaled = scaler_knn.fit_transform(df_numeric)

# Fit the KNN model
k = 2  # Number of neighbors
knn_model = NearestNeighbors(n_neighbors=k)
knn_model.fit(X_scaled)

# Calculate distances to k nearest neighbors for each instance
distances, _ = knn_model.kneighbors(X_scaled)

# Calculate the average distance to k nearest neighbors for each instance
avg_distances = np.mean(distances, axis=1)

# Set a threshold for anomaly detection (e.g., mean + 2 * standard deviation)
threshold = np.mean(avg_distances) + 2 * np.std(avg_distances)

# Identify anomalies based on the threshold
anomalies_indices = np.where(avg_distances > threshold)[0]
print("Indices of anomalies:", anomalies_indices)

mse_unsupervised = np.mean(avg_distances ** 2)
print("Mean Squared Error (Unsupervised KNN):", mse_unsupervised)
methods = ['KNN', 'Autoencoder']
accuracies = [mse_unsupervised, test_mse]

# Plotting the bar plot
plt.figure(figsize=(8, 6))
plt.bar(methods, accuracies, color=['blue', 'green'])
plt.title('Comparison of KNN and Autoencoder')
plt.xlabel('Method')
plt.ylabel('MSE')
plt.ylim(0, 1)
plt.show()
### Scatter Plot of Reconstruction Errors vs. Data Indices

plt.figure(figsize=(10, 5))
plt.scatter(range(len(reconstruction_errors)), reconstruction_errors, alpha=0.5)
plt.title('Scatter Plot of Reconstruction Errors vs. Data Indices')
plt.xlabel('Data Indices')
plt.ylabel('Reconstruction Error')
plt.show()
### Threshold Line on Reconstruction Error Scatter Plot
plt.figure(figsize=(10, 5))
plt.scatter(range(len(reconstruction_errors)), reconstruction_errors, alpha=0.5)
plt.axhline(y=threshold, color='r', linestyle='--', label='Threshold')
plt.title('Reconstruction Errors with Threshold Line')
plt.xlabel('Data Indices')
plt.ylabel('Reconstruction Error')
plt.legend()
plt.show()
